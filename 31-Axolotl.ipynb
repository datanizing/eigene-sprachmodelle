{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe31229-8f6b-48bc-a86d-af8e5466d11c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if GPU is available I used 4x NVIDIA GeForce RTX 3090 (rented 2.1.2-cuda12.1-cudnn8-devel)\n",
    "import torch\n",
    "print('GPU available?', torch.cuda.is_available())\n",
    "print('BF16 is supported?', torch.cuda.is_bf16_supported())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dee845b-f3cb-4b1e-bdd9-1a918eac140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88731672-9050-4034-8266-11aaace2a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5aa7d7-3b18-4c14-afd4-043c2c545259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Login to huggingface so you can push the model to hub later\n",
    "import sys\n",
    "stdout = sys.stdout\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74d0635-5033-4494-b7bd-ff6822103d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I noticed that when you use notebook_login() nothing gets printed after so we use sys \n",
    "sys.stdout = stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c3b088-45e7-484b-ae39-66beabc48da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#axolotl\n",
    "!git clone -b main --depth 1 https://github.com/OpenAccess-AI-Collective/axolotl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66927751-4fd6-4477-97fc-6ab08c9d9a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd axolotl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcccf8da-353b-4d70-8f55-5cfe08c7e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instaling what is needed inside axolotl file\n",
    "!pip install packaging\n",
    "!pip install -e '.[flash-attn,deepspeed]'\n",
    "!pip install -U git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d1a380-1e87-48fe-89fe-25331326014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training using the config.yml file and using deepspeed:zero3_bf16 the most aggressive optimization out of zero1,zero2,zero3 stages which partitions \n",
    "not only optimizer states but also gradients and parameters across GPUs. The bf16 indicate mixed precision training using bfloat16.\n",
    "For more information read axolotl's readme\n",
    "\"\"\"\n",
    "!accelerate launch -m axolotl.cli.train /folder/config.yml --deepspeed deepspeed_configs/zero3_bf16.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "axolotl",
   "language": "python",
   "name": "axolotl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
